# -------------------------------------------------------------------------------------------------------------
# ILUMEO - AI Marketing + ETL Autom√°tico
# Vers√£o FINAL ‚Äî Logs + Tabelas + Insights + Conte√∫do Multicanal
# + M√≥dulo YouTube ‚Üí Transcri√ß√£o REAL (Legenda ‚Üí Whisper) ‚Üí Blog (ADICIONAL)
# Melhorias:
#   ‚úÖ Transcri√ß√£o REAL: tenta legenda (r√°pido) e faz fallback para Whisper (robusto)
#   ‚úÖ Cache de transcri√ß√£o e blog por URL (evita custo duplicado)
#   ‚úÖ Bot√µes: "Transcrever / Gerar" e "Gerar novamente" + "Limpar m√≥dulo YouTube"
#   ‚úÖ Tratamento de erros e valida√ß√£o de URL
# Requisitos:
#   pip install yt-dlp youtube-transcript-api
#   ffmpeg instalado no sistema (para yt-dlp extrair √°udio)
# -------------------------------------------------------------------------------------------------------------

import os
import re
import json
import hashlib
import tempfile

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from crewai import Agent, Task, Crew

# Legendas do YouTube (quando dispon√≠veis)
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound

# Download de √°udio do YouTube
import yt_dlp

# ETL OFICIAL
from etl_ilumeo1 import executar_etl  # <<< ATEN√á√ÉO: usa etl_ilumeo1


# -------------------------------------------------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------------------------------------------------
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
st.set_page_config(page_title="ILUMEO - AI Marketing", layout="wide")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# -------------------------------------------------------------------------------------------------------------
# CSS ‚Äî PERSONALIZA√á√ÉO ILUMEO
# -------------------------------------------------------------------------------------------------------------
st.markdown(
    """
<style>
    html, body, [class*="css"] { font-family: 'Inter', sans-serif; color: #333333; }
    :root { --ilumeo-orange: #FF8A00; --sidebar-bg: #F7F7F7; --text-dark: #333333; --text-light: #666666; --border-soft: #E6E6E6; }
    body { background-color: white !important; color: var(--text-dark) !important; }
    section[data-testid="stSidebar"] { background-color: var(--sidebar-bg) !important; border-right: 1px solid var(--border-soft); padding-top: 2rem; }
    h1, h2, h3 { font-weight: 700 !important; color: var(--ilumeo-orange) !important; }
    p, label, span { color: var(--text-light) !important; font-weight: 400; }
    .stButton button { background-color: var(--ilumeo-orange) !important; color: white !important; border-radius: 6px !important; padding: 0.55rem 1.2rem !important; font-weight: 600 !important; border: none !important; }
    .stButton button:hover { background-color: #F59E0B !important; color: white !important; }
    .stFileUploader { background-color: white !important; border: 1px solid var(--border-soft); border-radius: 8px; padding: 10px; }
    hr { border: 0; border-top: 1px solid var(--border-soft); margin: 2rem 0; }
</style>
""",
    unsafe_allow_html=True,
)

# -------------------------------------------------------------------------------------------------------------
# ESTADOS
# -------------------------------------------------------------------------------------------------------------
defaults = {
    "json_etl": "",
    "insights": "",
    "conteudos_multicanais": "",
    "etl_logs": [],
    "t_simples": {},
    "t_multi": {},
    "t_matriz": {},
    "t_nota": {},

    # YOUTUBE
    "yt_url": "",
    "yt_transcricao": "",
    "yt_blog": "",
    "yt_origem_transcricao": "",  # "legenda" ou "whisper"
}

for k, v in defaults.items():
    if k not in st.session_state:
        st.session_state[k] = v


# -------------------------------------------------------------------------------------------------------------
# UTIL ‚Äî HELPERS
# -------------------------------------------------------------------------------------------------------------
def _hash_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()[:16]


def validar_url_youtube(url: str) -> bool:
    if not url:
        return False
    # cobre youtube.com/watch?v=... e youtu.be/...
    pattern = r"(https?://)?(www\.)?(youtube\.com/watch\?v=|youtu\.be/)[\w\-]+"
    return re.search(pattern, url) is not None


def extrair_video_id(url: str) -> str | None:
    # youtube.com/watch?v=ID
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    # youtu.be/ID
    m = re.search(r"youtu\.be/([^?&]+)", url)
    if m:
        return m.group(1)
    return None

# -------------------------------------------------------------------------------------------------------------
# IA ‚Äî INSIGHTS PROFUNDOS COM CRUZAMENTO
# -------------------------------------------------------------------------------------------------------------
def gerar_insights(json_text):
    agente = Agent(
        role="Analista de Mercado e Intelig√™ncia Competitiva S√™nior",
        goal=(
            "Realizar an√°lise profunda, cruzada e estrat√©gica do JSON, "
            "identificando padr√µes, clusters, motiva√ß√µes, barreiras e oportunidades."
        ),
        backstory=(
            "Especialista em comportamento do consumidor, marketing estrat√©gico, "
            "estat√≠stica de pesquisa e an√°lise de frequ√™ncia."
        ),
    )

    tarefa = Task(
        description=(
            "Voc√™ receber√° o JSON completo contendo tabelas de frequ√™ncias, m√∫ltiplas respostas, "
            "matriz de texto e matriz de notas. Realize uma AN√ÅLISE PROFUNDA REAL, com cruzamento de dados "
            "entre perguntas, compara√ß√µes entre categorias, interpreta√ß√£o de padr√µes e hip√≥teses de comportamento.\n\n"
            "Identifique:\n"
            "- Tend√™ncias e padr√µes fortes\n"
            "- Contradi√ß√µes e comportamentos divergentes\n"
            "- Barreiras, gatilhos e drivers de decis√£o\n"
            "- Oportunidades estrat√©gicas para marketing\n"
            "- Rela√ß√µes ocultas entre respostas\n"
            "- Segmenta√ß√µes impl√≠citas ou grupos naturais\n\n"
            "Use linguagem clara, humana, estrat√©gica e orientada a marketing.\n\n"
            "JSON:\n"
            f"{json_text}"
        ),
        expected_output="Insight completo, estrat√©gico, profundo e humanizado.",
        agent=agente,
    )

    equipe = Crew(agents=[agente], tasks=[tarefa])
    resultado = equipe.kickoff()
    return resultado.raw

# -------------------------------------------------------------------------------------------------------------
# IA ‚Äî CONTE√öDOS MULTICANAIS
# -------------------------------------------------------------------------------------------------------------
def gerar_conteudos_multicanais(insights):
    agente = Agent(
        role="Especialista em Conte√∫do Multicanal baseado em Insights de Dados",
        goal="Transformar insights profundos em 4 conte√∫dos distintos para canais diferentes.",
        backstory="Especialista em branding, marketing, jornalismo e escrita executiva.",
    )

    tarefa = Task(
        description=(
            "A partir dos insights fornecidos, gere QUATRO vers√µes de conte√∫do distintas:\n\n"
            "### 1) LINKEDIN\n"
            "- Tom institucional\n"
            "- Par√°grafos curtos\n"
            "- Abertura forte\n"
            "- Dados claros\n"
            "- CTA leve no final\n\n"
            "### 2) BLOG\n"
            "- Artigo estruturado\n"
            "- T√≠tulo forte\n"
            "- Subt√≠tulos organizados\n"
            "- Interpreta√ß√£o + contexto\n"
            "- Conclus√£o anal√≠tica\n\n"
            "- Cite Diego Senise CEO da Ilumeo em algum trecho do texto falando de algum insights relevante\n\n"
            "- Para realiza√ß√£o de estudo aprofundado, levantamento, pesquisa entre outras, cite a ILUMEO.\n\n"
            "### 3) ONE PAGE EXECUTIVA\n"
            "- Somente bullets\n"
            "- M√°ximo 12 palavras por bullet\n"
            "- Se√ß√µes: Dados / Achados / Oportunidades / Implica√ß√µes / Pr√≥ximos Passos\n\n"
            "### 4) NOT√çCIA JORNAL√çSTICA (Release)\n"
            "- Tom factual, objetivo e neutro\n"
            "- Narra√ß√£o em pir√¢mide invertida\n"
            "- Sem opini√£o pessoal\n\n"
            "- Cite Diego Senise CEO da Ilumeo em algum trecho do texto falando de algum insights relevante\n\n"
            "- Para realiza√ß√£o de estudo aprofundado, levantamento, pesquisa entre outras, cite a ILUMEO.\n\n"
            "INSIGHTS A TRANSFORMAR:\n"
            f"{insights}"
        ),
        expected_output="Documento contendo as quatro vers√µes, separadas e prontas para copiar.",
        agent=agente,
    )

    equipe = Crew(agents=[agente], tasks=[tarefa])
    resultado = equipe.kickoff()
    return resultado.raw

# -------------------------------------------------------------------------------------------------------------
# YOUTUBE ‚Üí TRANSCRI√á√ÉO (Legenda ‚Üí Whisper)
# -------------------------------------------------------------------------------------------------------------
def _transcrever_por_legenda(url: str) -> str:
    video_id = extrair_video_id(url)
    if not video_id:
        raise ValueError("N√£o foi poss√≠vel extrair o ID do v√≠deo a partir da URL.")

    # tenta PT/BR e depois EN
    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=["pt", "pt-BR", "pt-PT", "en"])
    return " ".join([item.get("text", "") for item in transcript]).strip()


def _transcrever_por_whisper(url: str) -> str:
    with tempfile.TemporaryDirectory() as tmpdir:
        audio_path = os.path.join(tmpdir, "audio.mp3")

        ydl_opts = {
            "format": "bestaudio/best",
            "outtmpl": audio_path,
            "quiet": True,
            "noplaylist": True,
            "postprocessors": [
                {
                    "key": "FFmpegExtractAudio",
                    "preferredcodec": "mp3",
                    "preferredquality": "192",
                }
            ],
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])

        with open(audio_path, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                file=audio_file,
                model="whisper-1",
                language="pt",
            )
    return transcription.text.strip()


@st.cache_data(show_spinner=False)
def transcrever_video_youtube_cacheada(url: str) -> dict:
    """
    Retorna um dict:
      { "texto": "...", "origem": "legenda"|"whisper" }
    Cacheado por URL para evitar custo repetido.
    """
    # 1) tenta legenda
    try:
        texto = _transcrever_por_legenda(url)
        if texto:
            return {"texto": texto, "origem": "legenda"}
    except (TranscriptsDisabled, NoTranscriptFound, ValueError):
        pass
    except Exception:
        # qualquer outra falha na legenda ‚Üí fallback
        pass

    # 2) fallback whisper
    texto = _transcrever_por_whisper(url)
    return {"texto": texto, "origem": "whisper"}


# -------------------------------------------------------------------------------------------------------------
# IA ‚Äî TRANSCRI√á√ÉO ‚Üí BLOG (cache por hash)
# -------------------------------------------------------------------------------------------------------------
@st.cache_data(show_spinner=False)
def gerar_blog_a_partir_transcricao_cacheado(transcricao: str) -> str:
    """
    Cache por conte√∫do (transcri√ß√£o) evita gerar o mesmo blog novamente
    mesmo que a URL seja reprocessada.
    """
    _ = _hash_text(transcricao)  # apenas para "amarrar" o cache ao conte√∫do

    resposta = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": f"""
A partir da transcri√ß√£o abaixo, gere um BLOG POST profissional
alinhado ao posicionamento da marca ILUMEO.

Regras:
- N√£o mencionar que veio de v√≠deo
- Linguagem estrat√©gica e institucional
- Estrutura:
    ‚Ä¢ T√≠tulo
    ‚Ä¢ Introdu√ß√£o
    ‚Ä¢ Subt√≠tulos
    ‚Ä¢ Desenvolvimento
    ‚Ä¢ Conclus√£o

TRANSCRI√á√ÉO:
{transcricao}
""".strip(),
            }
        ],
        temperature=0.1,
    )

    return resposta.choices[0].message.content


def limpar_modulo_youtube():
    st.session_state["yt_transcricao"] = ""
    st.session_state["yt_blog"] = ""
    st.session_state["yt_origem_transcricao"] = ""


# -------------------------------------------------------------------------------------------------------------
# SIDEBAR
# -------------------------------------------------------------------------------------------------------------
def sidebar():
    st.image("logo.png", width=170)

    st.markdown("### üìÇ Enviar arquivo Excel")
    arquivo = st.file_uploader("Upload", type=["xlsx"])

    st.markdown("---")
    st.markdown("### üé• YouTube ‚Üí Blog")

    st.session_state["yt_url"] = st.text_input(
        "Cole a URL do v√≠deo do YouTube",
        value=st.session_state["yt_url"],
        placeholder="Ex: https://www.youtube.com/watch?v=XXXX",
    )

    col_a, col_b = st.columns(2)
    with col_a:
        if st.button("üßπ Limpar YouTube"):
            limpar_modulo_youtube()
            st.rerun()
    with col_b:
        if st.button("üîÑ Limpar cache"):
            st.cache_data.clear()
            limpar_modulo_youtube()
            st.rerun()

    return arquivo


# -------------------------------------------------------------------------------------------------------------
# TELA PRINCIPAL ‚Äî FLUXO ORIGINAL + M√ìDULO ADICIONAL
# -------------------------------------------------------------------------------------------------------------
def main():
    with st.sidebar:
        arquivo = sidebar()

    st.title("üìä ILUMEO ‚Äî AI Marketing")
    st.markdown("Aqui, a Intelig√™ncia Artificial transforma seus dados em **insights**.\n")

    # ---------------------------------------------------------------------
    # M√ìDULO YOUTUBE (INDEPENDENTE DO ETL)
    # ---------------------------------------------------------------------
    if st.session_state["yt_url"]:
        st.markdown("---")
        st.subheader("üé• Conte√∫do Gerado a partir de V√≠deo do YouTube")

        url = st.session_state["yt_url"].strip()

        if not validar_url_youtube(url):
            st.error("URL inv√°lida. Cole uma URL v√°lida do YouTube (youtube.com/watch?v=... ou youtu.be/...).")
        else:
            col1, col2, col3 = st.columns(3)

            with col1:
                gerar_tudo = st.button("üéôÔ∏è Transcrever + ‚úçÔ∏è Gerar Blog", use_container_width=True)

            with col2:
                transcrever_apenas = st.button("üéôÔ∏è S√≥ Transcrever", use_container_width=True)

            with col3:
                regerar_blog = st.button("‚úçÔ∏è Gerar Blog novamente", use_container_width=True)

            # A√ß√µes
            if transcrever_apenas or gerar_tudo:
                with st.spinner("üéôÔ∏è Transcrevendo v√≠deo (Legenda ‚Üí Whisper)..."):
                    try:
                        out = transcrever_video_youtube_cacheada(url)
                        st.session_state["yt_transcricao"] = out["texto"]
                        st.session_state["yt_origem_transcricao"] = out["origem"]
                    except Exception as e:
                        st.error(f"Erro ao transcrever o v√≠deo: {e}")

            # Se pediu tudo e temos transcri√ß√£o
            if gerar_tudo and st.session_state["yt_transcricao"]:
                with st.spinner("‚úçÔ∏è Gerando blog post automaticamente..."):
                    try:
                        st.session_state["yt_blog"] = gerar_blog_a_partir_transcricao_cacheado(
                            st.session_state["yt_transcricao"]
                        )
                    except Exception as e:
                        st.error(f"Erro ao gerar o blog post: {e}")

            # Se pediu regerar (ignora cache do blog: limpa cache e gera)
            if regerar_blog and st.session_state["yt_transcricao"]:
                with st.spinner("‚úçÔ∏è Gerando blog post novamente (sem cache)..."):
                    try:
                        # truque: limpa cache apenas do blog (mais simples: limpa tudo)
                        # se preferir granularidade, d√° para separar em outro cache.
                        st.cache_data.clear()
                        st.session_state["yt_blog"] = gerar_blog_a_partir_transcricao_cacheado(
                            st.session_state["yt_transcricao"]
                        )
                    except Exception as e:
                        st.error(f"Erro ao gerar o blog post: {e}")

            # Exibi√ß√£o
            if st.session_state["yt_transcricao"]:
                origem = st.session_state.get("yt_origem_transcricao", "")
                if origem:
                    st.caption(f"Transcri√ß√£o obtida via: **{origem.upper()}**")

                st.text_area(
                    "üìù Transcri√ß√£o do V√≠deo",
                    st.session_state["yt_transcricao"],
                    height=250,
                )

            if st.session_state["yt_blog"]:
                st.subheader("‚úçÔ∏è Blog Post Gerado")
                st.markdown(st.session_state["yt_blog"])

    # ---------------------------------------------------------------------
    # UPLOAD ‚Üí ETL ‚Üí JSON (FLUXO ORIGINAL INALTERADO)
    # ---------------------------------------------------------------------
    if arquivo:
        with st.spinner("üîÑ Rodando ETL ILUMEO..."):
            os.makedirs("temp", exist_ok=True)
            caminho = os.path.join("temp", arquivo.name)

            with open(caminho, "wb") as f:
                f.write(arquivo.getbuffer())

            try:
                df, t_simples, t_multi, t_matriz, t_nota, logs = executar_etl(caminho)

                st.session_state["etl_logs"] = logs
                st.session_state["t_simples"] = t_simples
                st.session_state["t_multi"] = t_multi
                st.session_state["t_matriz"] = t_matriz
                st.session_state["t_nota"] = t_nota

                with open("resultado_pesquisa.json", "r", encoding="utf-8") as f:
                    st.session_state["json_etl"] = f.read()

                st.success("ETL conclu√≠do! JSON carregado com sucesso.")

            except Exception as e:
                st.error(f"Erro durante o ETL: {e}")
                return

        # ------------------- LOGS -------------------
        st.subheader("üìÑ Log da Execu√ß√£o do ETL")
        with st.expander("Ver detalhes"):
            for linha in st.session_state["etl_logs"]:
                st.markdown(f"- {linha}")

        # ------------------- TABELAS -------------------
        st.subheader("üìä Tabelas de Frequ√™ncia")

        with st.expander("üü¶ Perguntas Simples"):
            for pergunta, tabela in st.session_state["t_simples"].items():
                st.markdown(f"### {pergunta}")
                st.dataframe(tabela)

        with st.expander("üüß Multirresposta"):
            for pergunta, tabela in st.session_state["t_multi"].items():
                st.markdown(f"### {pergunta}")
                st.dataframe(tabela)

        with st.expander("üü© Matriz (Texto)"):
            for pergunta, meios in st.session_state["t_matriz"].items():
                st.markdown(f"## {pergunta}")
                for meio, tabela in meios.items():
                    st.markdown(f"**{meio}**")
                    st.dataframe(tabela)

        with st.expander("üü™ Matriz (Nota)"):
            for pergunta, marcas in st.session_state["t_nota"].items():
                st.markdown(f"## {pergunta}")
                for marca, tabela in marcas.items():
                    st.markdown(f"**{marca}**")
                    st.dataframe(tabela)

        # ---------------------------------------------------------------------
        # GERAR INSIGHT PROFUNDO
        # ---------------------------------------------------------------------
        with st.spinner("üß† Analisando dados profundamente e cruzando informa√ß√µes..."):
            st.session_state["insights"] = gerar_insights(st.session_state["json_etl"])

        st.subheader("üß† Insight Profundo da Pesquisa")
        st.markdown(st.session_state["insights"])

        st.markdown("---")

        # ---------------------------------------------------------------------
        # GERAR CONTE√öDOS MULTICANAIS AUTOMATICAMENTE
        # ---------------------------------------------------------------------
        st.subheader("‚úçÔ∏è Conte√∫do Multicanal Gerado Automaticamente")

        if not st.session_state["conteudos_multicanais"]:
            with st.spinner("‚úçÔ∏è Criando textos completos para todos os canais..."):
                st.session_state["conteudos_multicanais"] = gerar_conteudos_multicanais(
                    st.session_state["insights"]
                )
            st.rerun()

        st.markdown(st.session_state["conteudos_multicanais"])


if __name__ == "__main__":
    main()